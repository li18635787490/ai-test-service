"""
API è·¯ç”± - éœ€æ±‚åˆ†æä¸æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ
"""
from typing import Optional

from fastapi import APIRouter, HTTPException
from fastapi.responses import PlainTextResponse
from pydantic import BaseModel, Field

from app.models import RequirementAnalysisResult, TestCaseGenerationResult
from app.services.requirement_analyzer import RequirementAnalyzer
from app.services.document_parser import DocumentParser
from app.routers.documents import get_document_path, get_document_info

router = APIRouter(prefix="/requirements", tags=["éœ€æ±‚åˆ†æ"])


class AnalyzeRequest(BaseModel):
    """éœ€æ±‚åˆ†æè¯·æ±‚"""
    document_id: str = Field(..., description="æ–‡æ¡£ ID")
    ai_provider: Optional[str] = Field(default=None, description="AI æä¾›å•†")


class GenerateTestCasesRequest(BaseModel):
    """ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹è¯·æ±‚"""
    document_id: str = Field(..., description="æ–‡æ¡£ ID")
    ai_provider: Optional[str] = Field(default=None, description="AI æä¾›å•†")


@router.post("/analyze", response_model=RequirementAnalysisResult)
async def analyze_requirements(request: AnalyzeRequest):
    """
    åˆ†æéœ€æ±‚æ–‡æ¡£

    å¯¹éœ€æ±‚æ–‡æ¡£è¿›è¡Œå…¨é¢åˆ†æï¼ŒåŒ…æ‹¬ï¼š
    - éœ€æ±‚å®Œæ•´æ€§æ£€æŸ¥
    - åœºæ™¯è¦†ç›–æ£€æŸ¥
    - æè¿°è´¨é‡æ£€æŸ¥
    - å¯æµ‹è¯•æ€§æ£€æŸ¥

    è¿”å›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œæ”¹è¿›å»ºè®®ã€‚
    """
    try:
        # è·å–æ–‡æ¡£å†…å®¹
        doc_path = get_document_path(request.document_id)
        content, _ = await DocumentParser.parse(doc_path)

        # åˆ†æéœ€æ±‚
        analyzer = RequirementAnalyzer(request.ai_provider)
        result = await analyzer.analyze_requirements(content)

        return result

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"éœ€æ±‚åˆ†æå¤±è´¥: {str(e)}")


@router.post("/generate-testcases", response_model=TestCaseGenerationResult)
async def generate_test_cases(request: GenerateTestCasesRequest):
    """
    æ ¹æ®éœ€æ±‚æ–‡æ¡£ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹

    è‡ªåŠ¨ç”ŸæˆåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…æ‹¬ï¼š
    - æ­£å‘æµ‹è¯•ç”¨ä¾‹
    - åå‘æµ‹è¯•ç”¨ä¾‹
    - è¾¹ç•Œæµ‹è¯•ç”¨ä¾‹
    - å¼‚å¸¸æµ‹è¯•ç”¨ä¾‹

    æ¯ä¸ªç”¨ä¾‹åŒ…å«è¯¦ç»†çš„æµ‹è¯•æ­¥éª¤å’Œé¢„æœŸç»“æœã€‚
    """
    try:
        # è·å–æ–‡æ¡£å†…å®¹
        doc_path = get_document_path(request.document_id)
        content, _ = await DocumentParser.parse(doc_path)

        # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        analyzer = RequirementAnalyzer(request.ai_provider)
        result = await analyzer.generate_test_cases(content, request.document_id)

        return result

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.get("/generate-testcases/export")
async def export_test_cases(document_id: str, format: str = "markdown", ai_provider: Optional[str] = None):
    """
    ç”Ÿæˆå¹¶å¯¼å‡ºæµ‹è¯•ç”¨ä¾‹

    - **document_id**: æ–‡æ¡£ ID
    - **format**: å¯¼å‡ºæ ¼å¼ (markdown / csv / json)
    - **ai_provider**: AI æä¾›å•†
    """
    try:
        # è·å–æ–‡æ¡£å†…å®¹
        doc_path = get_document_path(document_id)
        doc_info = get_document_info(document_id)
        content, _ = await DocumentParser.parse(doc_path)

        # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        analyzer = RequirementAnalyzer(ai_provider)
        result = await analyzer.generate_test_cases(content, document_id)

        if format == "markdown":
            output = _export_markdown(result, doc_info.filename)
            return PlainTextResponse(content=output, media_type="text/markdown")
        elif format == "csv":
            output = _export_csv(result)
            return PlainTextResponse(content=output, media_type="text/csv")
        else:
            return result

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {str(e)}")


def _export_markdown(result: TestCaseGenerationResult, filename: str) -> str:
    """å¯¼å‡ºä¸º Markdown æ ¼å¼"""
    lines = [
        f"# æµ‹è¯•ç”¨ä¾‹æ–‡æ¡£",
        f"",
        f"**æºæ–‡æ¡£**: {filename}",
        f"**ç”Ÿæˆæ—¶é—´**: {result.generated_at.strftime('%Y-%m-%d %H:%M:%S')}",
        f"**ç”¨ä¾‹æ€»æ•°**: {result.total_cases}",
        f"",
        f"## è¦†ç›–æƒ…å†µ",
        f"",
        f"{result.coverage_summary}",
        f"",
        f"---",
        f"",
        f"## æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨",
        f""
    ]

    for tc in result.test_cases:
        priority_emoji = {"P0": "ğŸ”´", "P1": "ğŸŸ ", "P2": "ğŸŸ¡", "P3": "ğŸŸ¢"}
        emoji = priority_emoji.get(tc.priority.value, "âšª")

        lines.append(f"### {tc.case_id}: {tc.title}")
        lines.append(f"")
        lines.append(f"| å±æ€§ | å€¼ |")
        lines.append(f"|------|------|")
        lines.append(f"| **ä¼˜å…ˆçº§** | {emoji} {tc.priority.value} |")
        lines.append(f"| **ç±»å‹** | {tc.case_type.value} |")
        if tc.requirement_id:
            lines.append(f"| **å…³è”éœ€æ±‚** | {tc.requirement_id} |")
        if tc.precondition:
            lines.append(f"| **å‰ç½®æ¡ä»¶** | {tc.precondition} |")
        if tc.test_data:
            lines.append(f"| **æµ‹è¯•æ•°æ®** | {tc.test_data} |")
        if tc.tags:
            lines.append(f"| **æ ‡ç­¾** | {', '.join(tc.tags)} |")

        lines.append(f"")
        lines.append(f"**æµ‹è¯•æ­¥éª¤:**")
        lines.append(f"")
        lines.append(f"| æ­¥éª¤ | æ“ä½œ | é¢„æœŸç»“æœ |")
        lines.append(f"|------|------|----------|")

        for step in tc.steps:
            lines.append(f"| {step.step_number} | {step.action} | {step.expected_result} |")

        lines.append(f"")
        lines.append(f"---")
        lines.append(f"")

    return "\n".join(lines)


def _export_csv(result: TestCaseGenerationResult) -> str:
    """å¯¼å‡ºä¸º CSV æ ¼å¼"""
    lines = ["ç”¨ä¾‹ç¼–å·,æ ‡é¢˜,ä¼˜å…ˆçº§,ç±»å‹,å…³è”éœ€æ±‚,å‰ç½®æ¡ä»¶,æµ‹è¯•æ­¥éª¤,é¢„æœŸç»“æœ,æµ‹è¯•æ•°æ®,æ ‡ç­¾"]

    for tc in result.test_cases:
        steps_str = "; ".join([f"{s.step_number}. {s.action}" for s in tc.steps])
        expected_str = "; ".join([f"{s.step_number}. {s.expected_result}" for s in tc.steps])
        tags_str = "|".join(tc.tags)

        line = f'"{tc.case_id}","{tc.title}","{tc.priority.value}","{tc.case_type.value}",'
        line += f'"{tc.requirement_id or ""}","{tc.precondition or ""}",'
        line += f'"{steps_str}","{expected_str}","{tc.test_data or ""}","{tags_str}"'
        lines.append(line)

    return "\n".join(lines)


# ============ éœ€æ±‚åˆ†æç»“æœå¯¼å‡º ============

@router.get("/analyze/export")
async def export_analysis(document_id: str, format: str = "markdown", ai_provider: Optional[str] = None):
    """
    å¯¼å‡ºéœ€æ±‚åˆ†æç»“æœ

    - **document_id**: æ–‡æ¡£ ID
    - **format**: å¯¼å‡ºæ ¼å¼ (markdown / json)
    - **ai_provider**: AI æä¾›å•†
    """
    try:
        # è·å–æ–‡æ¡£å†…å®¹
        doc_path = get_document_path(document_id)
        doc_info = get_document_info(document_id)
        content, _ = await DocumentParser.parse(doc_path)

        # åˆ†æéœ€æ±‚
        analyzer = RequirementAnalyzer(ai_provider)
        result = await analyzer.analyze_requirements(content)

        if format == "markdown":
            output = _export_analysis_markdown(result, doc_info.filename)
            return PlainTextResponse(content=output, media_type="text/markdown")
        else:
            return result

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {str(e)}")


def _export_analysis_markdown(result: RequirementAnalysisResult, filename: str) -> str:
    """å¯¼å‡ºéœ€æ±‚åˆ†æç»“æœä¸º Markdown æ ¼å¼"""
    from datetime import datetime

    # è¯„åˆ†é¢œè‰²å‡½æ•°
    def get_score_emoji(score):
        if score >= 80:
            return "ğŸŸ¢"
        elif score >= 60:
            return "ğŸŸ¡"
        else:
            return "ğŸ”´"

    lines = [
        f"# ğŸ“‹ éœ€æ±‚åˆ†ææŠ¥å‘Š",
        f"",
        f"| é¡¹ç›® | ä¿¡æ¯ |",
        f"|------|------|",
        f"| **æºæ–‡æ¡£** | {filename} |",
        f"| **åˆ†ææ—¶é—´** | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |",
        f"| **éœ€æ±‚æ€»æ•°** | {result.total_requirements} ä¸ªåŠŸèƒ½ç‚¹ |",
        f"",
        f"---",
        f"",
        f"## ğŸ“Š è¯„åˆ†æ€»è§ˆ",
        f"",
        f"| ç»´åº¦ | å¾—åˆ† | çŠ¶æ€ |",
        f"|------|:----:|:----:|",
        f"| ğŸ¯ å®Œæ•´æ€§ | {result.completeness_score} | {get_score_emoji(result.completeness_score)} |",
        f"| ğŸ“‹ åœºæ™¯è¦†ç›– | {result.scenario_coverage_score} | {get_score_emoji(result.scenario_coverage_score)} |",
        f"| ğŸ“ æè¿°è´¨é‡ | {result.description_quality_score} | {get_score_emoji(result.description_quality_score)} |",
        f"| âœ… å¯æµ‹è¯•æ€§ | {result.testability_score} | {get_score_emoji(result.testability_score)} |",
        f"| **ğŸ† ç»¼åˆå¾—åˆ†** | **{result.overall_score}** | {get_score_emoji(result.overall_score)} |",
        f"",
        f"---",
        f"",
        f"## ğŸ“ åˆ†ææ€»ç»“",
        f"",
        f"> {result.summary}",
        f"",
        f"---",
        f"",
        f"## ğŸ” éœ€æ±‚è¯¦ç»†åˆ†æ",
        f""
    ]

    for idx, req in enumerate(result.analyzed_requirements, 1):
        issue_count = len(req.issues)
        status = f"âŒ {issue_count} ä¸ªé—®é¢˜" if issue_count > 0 else "âœ… å®Œæ•´"

        lines.append(f"### {idx}. {req.req_id}: {req.title}")
        lines.append(f"")
        lines.append(f"| å±æ€§ | å€¼ |")
        lines.append(f"|------|------|")
        lines.append(f"| **ä¼˜å…ˆçº§** | {req.priority or 'æœªå®šä¹‰'} |")
        lines.append(f"| **çŠ¶æ€** | {status} |")
        lines.append(f"")
        lines.append(f"**éœ€æ±‚æè¿°ï¼š**")
        lines.append(f"> {req.description}")
        lines.append(f"")

        if req.issues:
            lines.append(f"#### ğŸ”´ å‘ç°çš„é—®é¢˜ ({len(req.issues)})")
            lines.append(f"")
            for issue in req.issues:
                # è§£æé—®é¢˜ç±»å‹
                import re
                match = re.match(r'^\[([^\]]+)\]\s*(.*)$', issue)
                if match:
                    lines.append(f"- **`{match.group(1)}`** {match.group(2)}")
                else:
                    lines.append(f"- {issue}")
            lines.append(f"")

        if req.suggestions:
            lines.append(f"#### ğŸ’¡ æ”¹è¿›å»ºè®® ({len(req.suggestions)})")
            lines.append(f"")
            for suggestion in req.suggestions:
                lines.append(f"- {suggestion}")
            lines.append(f"")

        lines.append(f"---")
        lines.append(f"")

    if result.improvement_suggestions:
        lines.append(f"## ğŸ“Œ æ•´ä½“æ”¹è¿›å»ºè®®")
        lines.append(f"")
        for idx, suggestion in enumerate(result.improvement_suggestions, 1):
            # è§£æä¼˜å…ˆçº§
            import re
            match = re.match(r'^\[([^\]]+)\]\s*(.*)$', suggestion)
            if match:
                priority = match.group(1)
                content = match.group(2)
                emoji = "ğŸ”´" if "é«˜" in priority else "ğŸŸ¡" if "ä¸­" in priority else "ğŸŸ¢"
                lines.append(f"{idx}. {emoji} **{priority}** - {content}")
            else:
                lines.append(f"{idx}. {suggestion}")
        lines.append(f"")

    lines.append(f"---")
    lines.append(f"")
    lines.append(f"*æŠ¥å‘Šç”± AI æ–‡æ¡£æ£€æµ‹æœåŠ¡è‡ªåŠ¨ç”Ÿæˆ*")

    return "\n".join(lines)

